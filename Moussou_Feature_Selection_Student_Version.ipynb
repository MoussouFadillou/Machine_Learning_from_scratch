{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Moussou_Feature Selection_Student_Version.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPcZrKllwfcd"
      },
      "source": [
        "## Feature Selection: Forward and Backward Feature Elimination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtFImfJJwfcj"
      },
      "source": [
        "In this notebook, we will go through implementing Wrapper Models for feature Selection. We will work on a  Linear Regression problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewBznQW2wfcj"
      },
      "source": [
        "#### import packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "from sklearn.model_selection import cross_val_score, train_test_split, KFold \n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s85yLMg-yDgj",
        "outputId": "1461a0f9-d3c3-46b3-c0a8-1b42bf6db29e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MlqgSYmwfck"
      },
      "source": [
        "### 1- Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "v9muIHouwfck",
        "scrolled": true,
        "outputId": "f0f3b02a-b96c-42dd-d476-6b9789a92f46"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/cancer.csv', index_col=0)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>avgAnnCount</th>\n",
              "      <th>avgDeathsPerYear</th>\n",
              "      <th>TARGET_deathRate</th>\n",
              "      <th>incidenceRate</th>\n",
              "      <th>medIncome</th>\n",
              "      <th>popEst2015</th>\n",
              "      <th>povertyPercent</th>\n",
              "      <th>studyPerCap</th>\n",
              "      <th>MedianAge</th>\n",
              "      <th>MedianAgeMale</th>\n",
              "      <th>MedianAgeFemale</th>\n",
              "      <th>AvgHouseholdSize</th>\n",
              "      <th>PercentMarried</th>\n",
              "      <th>PctNoHS18_24</th>\n",
              "      <th>PctHS18_24</th>\n",
              "      <th>PctSomeCol18_24</th>\n",
              "      <th>PctBachDeg18_24</th>\n",
              "      <th>PctHS25_Over</th>\n",
              "      <th>PctBachDeg25_Over</th>\n",
              "      <th>PctEmployed16_Over</th>\n",
              "      <th>PctUnemployed16_Over</th>\n",
              "      <th>PctPrivateCoverage</th>\n",
              "      <th>PctPrivateCoverageAlone</th>\n",
              "      <th>PctEmpPrivCoverage</th>\n",
              "      <th>PctPublicCoverage</th>\n",
              "      <th>PctPublicCoverageAlone</th>\n",
              "      <th>PctWhite</th>\n",
              "      <th>PctBlack</th>\n",
              "      <th>PctAsian</th>\n",
              "      <th>PctOtherRace</th>\n",
              "      <th>PctMarriedHouseholds</th>\n",
              "      <th>BirthRate</th>\n",
              "      <th>avgAnnCount-2</th>\n",
              "      <th>avgAnnCount-3</th>\n",
              "      <th>avgAnnCount-4</th>\n",
              "      <th>avgAnnCount-5</th>\n",
              "      <th>avgAnnCount-6</th>\n",
              "      <th>avgAnnCount-7</th>\n",
              "      <th>avgAnnCount-8</th>\n",
              "      <th>avgAnnCount-9</th>\n",
              "      <th>...</th>\n",
              "      <th>PctBlack-2</th>\n",
              "      <th>PctBlack-3</th>\n",
              "      <th>PctBlack-4</th>\n",
              "      <th>PctBlack-5</th>\n",
              "      <th>PctBlack-6</th>\n",
              "      <th>PctBlack-7</th>\n",
              "      <th>PctBlack-8</th>\n",
              "      <th>PctBlack-9</th>\n",
              "      <th>PctAsian-2</th>\n",
              "      <th>PctAsian-3</th>\n",
              "      <th>PctAsian-4</th>\n",
              "      <th>PctAsian-5</th>\n",
              "      <th>PctAsian-6</th>\n",
              "      <th>PctAsian-7</th>\n",
              "      <th>PctAsian-8</th>\n",
              "      <th>PctAsian-9</th>\n",
              "      <th>PctOtherRace-2</th>\n",
              "      <th>PctOtherRace-3</th>\n",
              "      <th>PctOtherRace-4</th>\n",
              "      <th>PctOtherRace-5</th>\n",
              "      <th>PctOtherRace-6</th>\n",
              "      <th>PctOtherRace-7</th>\n",
              "      <th>PctOtherRace-8</th>\n",
              "      <th>PctOtherRace-9</th>\n",
              "      <th>PctMarriedHouseholds-2</th>\n",
              "      <th>PctMarriedHouseholds-3</th>\n",
              "      <th>PctMarriedHouseholds-4</th>\n",
              "      <th>PctMarriedHouseholds-5</th>\n",
              "      <th>PctMarriedHouseholds-6</th>\n",
              "      <th>PctMarriedHouseholds-7</th>\n",
              "      <th>PctMarriedHouseholds-8</th>\n",
              "      <th>PctMarriedHouseholds-9</th>\n",
              "      <th>BirthRate-2</th>\n",
              "      <th>BirthRate-3</th>\n",
              "      <th>BirthRate-4</th>\n",
              "      <th>BirthRate-5</th>\n",
              "      <th>BirthRate-6</th>\n",
              "      <th>BirthRate-7</th>\n",
              "      <th>BirthRate-8</th>\n",
              "      <th>BirthRate-9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1397.0</td>\n",
              "      <td>469</td>\n",
              "      <td>164.9</td>\n",
              "      <td>489.8</td>\n",
              "      <td>61898</td>\n",
              "      <td>260131</td>\n",
              "      <td>11.2</td>\n",
              "      <td>499.748204</td>\n",
              "      <td>39.3</td>\n",
              "      <td>36.9</td>\n",
              "      <td>41.7</td>\n",
              "      <td>2.54</td>\n",
              "      <td>52.5</td>\n",
              "      <td>11.5</td>\n",
              "      <td>39.5</td>\n",
              "      <td>42.1</td>\n",
              "      <td>6.9</td>\n",
              "      <td>23.2</td>\n",
              "      <td>19.6</td>\n",
              "      <td>51.9</td>\n",
              "      <td>8.0</td>\n",
              "      <td>75.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>41.6</td>\n",
              "      <td>32.9</td>\n",
              "      <td>14.0</td>\n",
              "      <td>81.780529</td>\n",
              "      <td>2.594728</td>\n",
              "      <td>4.821857</td>\n",
              "      <td>1.843479</td>\n",
              "      <td>52.856076</td>\n",
              "      <td>6.118831</td>\n",
              "      <td>1951609.0</td>\n",
              "      <td>2.726398e+09</td>\n",
              "      <td>3.808778e+12</td>\n",
              "      <td>5.320862e+15</td>\n",
              "      <td>7.433245e+18</td>\n",
              "      <td>1.038424e+22</td>\n",
              "      <td>1.450679e+25</td>\n",
              "      <td>2.026598e+28</td>\n",
              "      <td>...</td>\n",
              "      <td>6.732615</td>\n",
              "      <td>17.469307</td>\n",
              "      <td>45.328106</td>\n",
              "      <td>117.614122</td>\n",
              "      <td>305.176694</td>\n",
              "      <td>791.850616</td>\n",
              "      <td>2054.637228</td>\n",
              "      <td>5331.225430</td>\n",
              "      <td>23.250306</td>\n",
              "      <td>112.109653</td>\n",
              "      <td>540.576725</td>\n",
              "      <td>2606.583721</td>\n",
              "      <td>12568.574225</td>\n",
              "      <td>60603.868891</td>\n",
              "      <td>292223.195619</td>\n",
              "      <td>1.409058e+06</td>\n",
              "      <td>3.398413</td>\n",
              "      <td>6.264902</td>\n",
              "      <td>11.549212</td>\n",
              "      <td>21.290724</td>\n",
              "      <td>39.248992</td>\n",
              "      <td>72.354674</td>\n",
              "      <td>133.384289</td>\n",
              "      <td>245.891073</td>\n",
              "      <td>2793.764757</td>\n",
              "      <td>147667.442010</td>\n",
              "      <td>7.805122e+06</td>\n",
              "      <td>4.125481e+08</td>\n",
              "      <td>2.180567e+10</td>\n",
              "      <td>1.152562e+12</td>\n",
              "      <td>6.091992e+13</td>\n",
              "      <td>3.219988e+15</td>\n",
              "      <td>37.440093</td>\n",
              "      <td>229.089604</td>\n",
              "      <td>1401.760576</td>\n",
              "      <td>8577.136107</td>\n",
              "      <td>52482.046553</td>\n",
              "      <td>321128.774915</td>\n",
              "      <td>1.964933e+06</td>\n",
              "      <td>1.202309e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>173.0</td>\n",
              "      <td>70</td>\n",
              "      <td>161.3</td>\n",
              "      <td>411.6</td>\n",
              "      <td>48127</td>\n",
              "      <td>43269</td>\n",
              "      <td>18.6</td>\n",
              "      <td>23.111234</td>\n",
              "      <td>33.0</td>\n",
              "      <td>32.2</td>\n",
              "      <td>33.7</td>\n",
              "      <td>2.34</td>\n",
              "      <td>44.5</td>\n",
              "      <td>6.1</td>\n",
              "      <td>22.4</td>\n",
              "      <td>64.0</td>\n",
              "      <td>7.5</td>\n",
              "      <td>26.0</td>\n",
              "      <td>22.7</td>\n",
              "      <td>55.9</td>\n",
              "      <td>7.8</td>\n",
              "      <td>70.2</td>\n",
              "      <td>53.8</td>\n",
              "      <td>43.6</td>\n",
              "      <td>31.1</td>\n",
              "      <td>15.3</td>\n",
              "      <td>89.228509</td>\n",
              "      <td>0.969102</td>\n",
              "      <td>2.246233</td>\n",
              "      <td>3.741352</td>\n",
              "      <td>45.372500</td>\n",
              "      <td>4.333096</td>\n",
              "      <td>29929.0</td>\n",
              "      <td>5.177717e+06</td>\n",
              "      <td>8.957450e+08</td>\n",
              "      <td>1.549639e+11</td>\n",
              "      <td>2.680875e+13</td>\n",
              "      <td>4.637914e+15</td>\n",
              "      <td>8.023592e+17</td>\n",
              "      <td>1.388081e+20</td>\n",
              "      <td>...</td>\n",
              "      <td>0.939160</td>\n",
              "      <td>0.910142</td>\n",
              "      <td>0.882021</td>\n",
              "      <td>0.854768</td>\n",
              "      <td>0.828358</td>\n",
              "      <td>0.802764</td>\n",
              "      <td>0.777961</td>\n",
              "      <td>0.753923</td>\n",
              "      <td>5.045561</td>\n",
              "      <td>11.333503</td>\n",
              "      <td>25.457684</td>\n",
              "      <td>57.183879</td>\n",
              "      <td>128.448293</td>\n",
              "      <td>288.524742</td>\n",
              "      <td>648.093677</td>\n",
              "      <td>1.455769e+03</td>\n",
              "      <td>13.997711</td>\n",
              "      <td>52.370359</td>\n",
              "      <td>195.935921</td>\n",
              "      <td>733.065158</td>\n",
              "      <td>2742.654452</td>\n",
              "      <td>10261.234432</td>\n",
              "      <td>38390.885152</td>\n",
              "      <td>143633.796942</td>\n",
              "      <td>2058.663796</td>\n",
              "      <td>93406.723998</td>\n",
              "      <td>4.238097e+06</td>\n",
              "      <td>1.922930e+08</td>\n",
              "      <td>8.724816e+09</td>\n",
              "      <td>3.958667e+11</td>\n",
              "      <td>1.796146e+13</td>\n",
              "      <td>8.149565e+14</td>\n",
              "      <td>18.775717</td>\n",
              "      <td>81.356978</td>\n",
              "      <td>352.527560</td>\n",
              "      <td>1527.535610</td>\n",
              "      <td>6618.957797</td>\n",
              "      <td>28680.576760</td>\n",
              "      <td>1.242757e+05</td>\n",
              "      <td>5.384984e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>102.0</td>\n",
              "      <td>50</td>\n",
              "      <td>174.7</td>\n",
              "      <td>349.7</td>\n",
              "      <td>49348</td>\n",
              "      <td>21026</td>\n",
              "      <td>14.6</td>\n",
              "      <td>47.560164</td>\n",
              "      <td>45.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>45.8</td>\n",
              "      <td>2.62</td>\n",
              "      <td>54.2</td>\n",
              "      <td>24.0</td>\n",
              "      <td>36.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.5</td>\n",
              "      <td>29.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>45.9</td>\n",
              "      <td>7.0</td>\n",
              "      <td>63.7</td>\n",
              "      <td>43.5</td>\n",
              "      <td>34.9</td>\n",
              "      <td>42.1</td>\n",
              "      <td>21.1</td>\n",
              "      <td>90.922190</td>\n",
              "      <td>0.739673</td>\n",
              "      <td>0.465898</td>\n",
              "      <td>2.747358</td>\n",
              "      <td>54.444868</td>\n",
              "      <td>3.729488</td>\n",
              "      <td>10404.0</td>\n",
              "      <td>1.061208e+06</td>\n",
              "      <td>1.082432e+08</td>\n",
              "      <td>1.104081e+10</td>\n",
              "      <td>1.126162e+12</td>\n",
              "      <td>1.148686e+14</td>\n",
              "      <td>1.171659e+16</td>\n",
              "      <td>1.195093e+18</td>\n",
              "      <td>...</td>\n",
              "      <td>0.547117</td>\n",
              "      <td>0.404688</td>\n",
              "      <td>0.299337</td>\n",
              "      <td>0.221411</td>\n",
              "      <td>0.163772</td>\n",
              "      <td>0.121138</td>\n",
              "      <td>0.089602</td>\n",
              "      <td>0.066277</td>\n",
              "      <td>0.217061</td>\n",
              "      <td>0.101128</td>\n",
              "      <td>0.047116</td>\n",
              "      <td>0.021951</td>\n",
              "      <td>0.010227</td>\n",
              "      <td>0.004765</td>\n",
              "      <td>0.002220</td>\n",
              "      <td>1.034235e-03</td>\n",
              "      <td>7.547978</td>\n",
              "      <td>20.736999</td>\n",
              "      <td>56.971967</td>\n",
              "      <td>156.522407</td>\n",
              "      <td>430.023135</td>\n",
              "      <td>1181.427634</td>\n",
              "      <td>3245.805027</td>\n",
              "      <td>8917.389410</td>\n",
              "      <td>2964.243692</td>\n",
              "      <td>161387.857618</td>\n",
              "      <td>8.786741e+06</td>\n",
              "      <td>4.783929e+08</td>\n",
              "      <td>2.604604e+10</td>\n",
              "      <td>1.418073e+12</td>\n",
              "      <td>7.720681e+13</td>\n",
              "      <td>4.203515e+15</td>\n",
              "      <td>13.909079</td>\n",
              "      <td>51.873742</td>\n",
              "      <td>193.462489</td>\n",
              "      <td>721.515996</td>\n",
              "      <td>2690.885118</td>\n",
              "      <td>10035.623263</td>\n",
              "      <td>3.742773e+04</td>\n",
              "      <td>1.395863e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>427.0</td>\n",
              "      <td>202</td>\n",
              "      <td>194.8</td>\n",
              "      <td>430.4</td>\n",
              "      <td>44243</td>\n",
              "      <td>75882</td>\n",
              "      <td>17.1</td>\n",
              "      <td>342.637253</td>\n",
              "      <td>42.8</td>\n",
              "      <td>42.2</td>\n",
              "      <td>43.4</td>\n",
              "      <td>2.52</td>\n",
              "      <td>52.7</td>\n",
              "      <td>20.2</td>\n",
              "      <td>41.2</td>\n",
              "      <td>36.1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>31.6</td>\n",
              "      <td>9.3</td>\n",
              "      <td>48.3</td>\n",
              "      <td>12.1</td>\n",
              "      <td>58.4</td>\n",
              "      <td>40.3</td>\n",
              "      <td>35.0</td>\n",
              "      <td>45.3</td>\n",
              "      <td>25.0</td>\n",
              "      <td>91.744686</td>\n",
              "      <td>0.782626</td>\n",
              "      <td>1.161359</td>\n",
              "      <td>1.362643</td>\n",
              "      <td>51.021514</td>\n",
              "      <td>4.603841</td>\n",
              "      <td>182329.0</td>\n",
              "      <td>7.785448e+07</td>\n",
              "      <td>3.324386e+10</td>\n",
              "      <td>1.419513e+13</td>\n",
              "      <td>6.061321e+15</td>\n",
              "      <td>2.588184e+18</td>\n",
              "      <td>1.105155e+21</td>\n",
              "      <td>4.719010e+23</td>\n",
              "      <td>...</td>\n",
              "      <td>0.612503</td>\n",
              "      <td>0.479361</td>\n",
              "      <td>0.375160</td>\n",
              "      <td>0.293610</td>\n",
              "      <td>0.229787</td>\n",
              "      <td>0.179837</td>\n",
              "      <td>0.140745</td>\n",
              "      <td>0.110151</td>\n",
              "      <td>1.348754</td>\n",
              "      <td>1.566387</td>\n",
              "      <td>1.819137</td>\n",
              "      <td>2.112671</td>\n",
              "      <td>2.453569</td>\n",
              "      <td>2.849473</td>\n",
              "      <td>3.309260</td>\n",
              "      <td>3.843238e+00</td>\n",
              "      <td>1.856796</td>\n",
              "      <td>2.530151</td>\n",
              "      <td>3.447693</td>\n",
              "      <td>4.697975</td>\n",
              "      <td>6.401664</td>\n",
              "      <td>8.723184</td>\n",
              "      <td>11.886587</td>\n",
              "      <td>16.197177</td>\n",
              "      <td>2603.194940</td>\n",
              "      <td>132818.948317</td>\n",
              "      <td>6.776624e+06</td>\n",
              "      <td>3.457536e+08</td>\n",
              "      <td>1.764087e+10</td>\n",
              "      <td>9.000641e+11</td>\n",
              "      <td>4.592263e+13</td>\n",
              "      <td>2.343042e+15</td>\n",
              "      <td>21.195350</td>\n",
              "      <td>97.580016</td>\n",
              "      <td>449.242856</td>\n",
              "      <td>2068.242577</td>\n",
              "      <td>9521.859503</td>\n",
              "      <td>43837.125013</td>\n",
              "      <td>2.018191e+05</td>\n",
              "      <td>9.291432e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57.0</td>\n",
              "      <td>26</td>\n",
              "      <td>144.4</td>\n",
              "      <td>350.1</td>\n",
              "      <td>49955</td>\n",
              "      <td>10321</td>\n",
              "      <td>12.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>48.3</td>\n",
              "      <td>47.8</td>\n",
              "      <td>48.9</td>\n",
              "      <td>2.34</td>\n",
              "      <td>57.8</td>\n",
              "      <td>14.9</td>\n",
              "      <td>43.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>33.4</td>\n",
              "      <td>15.0</td>\n",
              "      <td>48.2</td>\n",
              "      <td>4.8</td>\n",
              "      <td>61.6</td>\n",
              "      <td>43.9</td>\n",
              "      <td>35.1</td>\n",
              "      <td>44.0</td>\n",
              "      <td>22.7</td>\n",
              "      <td>94.104024</td>\n",
              "      <td>0.270192</td>\n",
              "      <td>0.665830</td>\n",
              "      <td>0.492135</td>\n",
              "      <td>54.027460</td>\n",
              "      <td>6.796657</td>\n",
              "      <td>3249.0</td>\n",
              "      <td>1.851930e+05</td>\n",
              "      <td>1.055600e+07</td>\n",
              "      <td>6.016921e+08</td>\n",
              "      <td>3.429645e+10</td>\n",
              "      <td>1.954897e+12</td>\n",
              "      <td>1.114292e+14</td>\n",
              "      <td>6.351462e+15</td>\n",
              "      <td>...</td>\n",
              "      <td>0.073004</td>\n",
              "      <td>0.019725</td>\n",
              "      <td>0.005330</td>\n",
              "      <td>0.001440</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.443330</td>\n",
              "      <td>0.295183</td>\n",
              "      <td>0.196542</td>\n",
              "      <td>0.130863</td>\n",
              "      <td>0.087133</td>\n",
              "      <td>0.058016</td>\n",
              "      <td>0.038629</td>\n",
              "      <td>2.572008e-02</td>\n",
              "      <td>0.242197</td>\n",
              "      <td>0.119194</td>\n",
              "      <td>0.058660</td>\n",
              "      <td>0.028868</td>\n",
              "      <td>0.014207</td>\n",
              "      <td>0.006992</td>\n",
              "      <td>0.003441</td>\n",
              "      <td>0.001693</td>\n",
              "      <td>2918.966429</td>\n",
              "      <td>157704.341819</td>\n",
              "      <td>8.520365e+06</td>\n",
              "      <td>4.603337e+08</td>\n",
              "      <td>2.487066e+10</td>\n",
              "      <td>1.343699e+12</td>\n",
              "      <td>7.259662e+13</td>\n",
              "      <td>3.922211e+15</td>\n",
              "      <td>46.194552</td>\n",
              "      <td>313.968540</td>\n",
              "      <td>2133.936595</td>\n",
              "      <td>14503.635908</td>\n",
              "      <td>98576.244063</td>\n",
              "      <td>669988.956897</td>\n",
              "      <td>4.553685e+06</td>\n",
              "      <td>3.094984e+07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 280 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   avgAnnCount  avgDeathsPerYear  ...   BirthRate-8   BirthRate-9\n",
              "0       1397.0               469  ...  1.964933e+06  1.202309e+07\n",
              "1        173.0                70  ...  1.242757e+05  5.384984e+05\n",
              "2        102.0                50  ...  3.742773e+04  1.395863e+05\n",
              "3        427.0               202  ...  2.018191e+05  9.291432e+05\n",
              "4         57.0                26  ...  4.553685e+06  3.094984e+07\n",
              "\n",
              "[5 rows x 280 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n9DKY3Rwfcl",
        "outputId": "e0aa4ab1-0ce8-441a-f373-74596fc7ef3e"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3047, 280)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtGWIjvEwfcl"
      },
      "source": [
        "### 2- Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34cKhGQZwfcl",
        "outputId": "d1095cec-4fd8-47d9-9c42-d96f6b5276e7"
      },
      "source": [
        "#Check Nan values\n",
        "for col in data.columns:\n",
        "    if data[col].isna().sum()!=0:\n",
        "        print(f\"Col: {col}, Number of nan value: {data[col].isna().sum()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Col: PctSomeCol18_24, Number of nan value: 2285\n",
            "Col: PctEmployed16_Over, Number of nan value: 152\n",
            "Col: PctPrivateCoverageAlone, Number of nan value: 609\n",
            "Col: PctSomeCol18_24-2, Number of nan value: 2285\n",
            "Col: PctSomeCol18_24-3, Number of nan value: 2285\n",
            "Col: PctSomeCol18_24-4, Number of nan value: 2285\n",
            "Col: PctSomeCol18_24-5, Number of nan value: 2285\n",
            "Col: PctSomeCol18_24-6, Number of nan value: 2285\n",
            "Col: PctSomeCol18_24-7, Number of nan value: 2285\n",
            "Col: PctSomeCol18_24-8, Number of nan value: 2285\n",
            "Col: PctSomeCol18_24-9, Number of nan value: 2285\n",
            "Col: PctEmployed16_Over-2, Number of nan value: 152\n",
            "Col: PctEmployed16_Over-3, Number of nan value: 152\n",
            "Col: PctEmployed16_Over-4, Number of nan value: 152\n",
            "Col: PctEmployed16_Over-5, Number of nan value: 152\n",
            "Col: PctEmployed16_Over-6, Number of nan value: 152\n",
            "Col: PctEmployed16_Over-7, Number of nan value: 152\n",
            "Col: PctEmployed16_Over-8, Number of nan value: 152\n",
            "Col: PctEmployed16_Over-9, Number of nan value: 152\n",
            "Col: PctPrivateCoverageAlone-2, Number of nan value: 609\n",
            "Col: PctPrivateCoverageAlone-3, Number of nan value: 609\n",
            "Col: PctPrivateCoverageAlone-4, Number of nan value: 609\n",
            "Col: PctPrivateCoverageAlone-5, Number of nan value: 609\n",
            "Col: PctPrivateCoverageAlone-6, Number of nan value: 609\n",
            "Col: PctPrivateCoverageAlone-7, Number of nan value: 609\n",
            "Col: PctPrivateCoverageAlone-8, Number of nan value: 609\n",
            "Col: PctPrivateCoverageAlone-9, Number of nan value: 609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-u9N4Pywfcl",
        "outputId": "6400ea22-49ea-407f-c6f9-aa5f92eb9374"
      },
      "source": [
        "for col in data.columns:\n",
        "    if data[col].isna().sum()!=0:\n",
        "        data[col][data[col].isna()] = data[col].mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZN3tMyNwfcm"
      },
      "source": [
        "y = data['TARGET_deathRate']\n",
        "X = data.drop(columns=['TARGET_deathRate'])\n",
        "normalized_X = (X - X.mean()) / X.std()\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(normalized_X, y, test_size = .2, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXQ1eYmDwfcm"
      },
      "source": [
        "### 3- Fit a LR model on all features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWYQpxQ4wfcm",
        "outputId": "e8f775b9-750c-44ce-e5ab-be5dff88458e"
      },
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(X_train, Y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "print('RMSE: ', np.sqrt(mean_squared_error(y_pred, Y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE:  112255.8675969501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcZyAszwwfcn",
        "outputId": "acea45d3-e58a-448b-d936-8fc7a74b7264"
      },
      "source": [
        "kf = KFold(n_splits=5, random_state= 42)\n",
        "print(kf)  \n",
        "\n",
        "# KFold training helper function\n",
        "def KFold_train(model, X, Y, kf):\n",
        "    scores = []\n",
        "    for train_index, val_index in kf.split(X): #we split the train to KFold cross validate\n",
        "        x_train, x_val = X[train_index], X[val_index]\n",
        "        y_train, y_val = Y[train_index], Y[val_index]\n",
        "        model.fit(x_train, y_train)\n",
        "        pred_y = model.predict(x_val)\n",
        "        scores.append(mean_squared_error(y_val, pred_y)**.5)\n",
        "    \n",
        "    return np.mean(np.array(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KFold(n_splits=5, random_state=42, shuffle=False)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emb8Aovjwfcn"
      },
      "source": [
        "### Feature selection greedy methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoPWQRTfwfcn",
        "outputId": "dd326be8-df0c-4156-f8f6-6998cb383ae7"
      },
      "source": [
        "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2437, 279), (610, 279), (2437,), (610,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEwDRUVpwfcn"
      },
      "source": [
        "## 4 -  Forward Search\n",
        "<img src='forward.png'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6wrQT22wfcn",
        "scrolled": false
      },
      "source": [
        "def forward_search(model, K, X, Y):\n",
        "    \n",
        "    rmse_losses = []\n",
        "    best_losses = []\n",
        "    set_features = []\n",
        "    all_scores = []\n",
        "    \n",
        "    ### get all the features of the data\n",
        "    features =  list(X.columns)\n",
        "    i = 1 # this index handles the case of one feature where reshaping is needed (do not care about it)\n",
        "    while len(set_features) <= K:\n",
        "        rmse_losses = []\n",
        "\n",
        "        for feature in features:\n",
        "\n",
        "            # Extract X with the subset of features\n",
        "            new_feature = [feature] + set_features\n",
        "\n",
        "            # Extract X with the subset of new features\n",
        "            new_data = X[new_feature]\n",
        "\n",
        "            #split the new_data in train and validation(test_size = 20%)\n",
        "            x_train, x_val, y_train, y_val = train_test_split(new_data.values,Y.values,test_size=0.2,random_state=120)\n",
        "            y_train = y_train.reshape(-1, 1)\n",
        "            y_val = y_val.reshape(-1, 1)\n",
        "\n",
        "            if i == 1:\n",
        "                x_train = x_train.reshape(-1, 1)\n",
        "                x_val = x_val.reshape(-1, 1)\n",
        "\n",
        "            #Cross validation using the KFold_train function\n",
        "            \n",
        "            scores= KFold_train(model, x_train, y_train, kf)\n",
        "            \n",
        "            # Fit the model on the training set\n",
        "            #None ## \n",
        "            model.fit(x_train, y_train)\n",
        "\n",
        "            \n",
        "            #Make Prediction on the validation set\n",
        "            #ypred =x_val None \n",
        "            pred_y=model.predict(x_val)\n",
        "\n",
        "            #Calculate the RMSE on the validation set\n",
        "            rmse_loss = np.sqrt(mean_squared_error(pred_y, y_val))\n",
        "\n",
        "            rmse_losses.append(rmse_loss)\n",
        "        \n",
        "        i += 1\n",
        "        #select features that give the best RMSE\n",
        "        present_best_feature =features[np.argmin(rmse_losses)]\n",
        "\n",
        "        #remove that present best feature from the initial set of input features, we will not include it again\n",
        "        \n",
        "        features.remove(present_best_feature)\n",
        "        \n",
        "        #add the present best feature to set_features\n",
        "         ### replace with your code###\n",
        "        set_features.append(present_best_feature)\n",
        "        \n",
        "        all_scores.append(np.mean(scores))\n",
        "        best_losses.append(np.min(rmse_losses))\n",
        "        print('K=',len(set_features),':',set_features, ' || RMSE: ', np.min(rmse_losses) )\n",
        "    return set_features, all_scores,best_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCof8BYJHS0J"
      },
      "source": [
        "a = [1,2,3,4,5]\n",
        "a.remove(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5DFNlvvwfco",
        "outputId": "d861b5fe-a899-498f-9735-0c100790b2e1"
      },
      "source": [
        "### Let us fix a value of K, in this case k = d**.5\n",
        "K = int(X.shape[1]**.5)\n",
        "lr = LinearRegression()\n",
        "best_features, scores, best_losses = forward_search(lr, K, X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K= 1 : ['PctBachDeg25_Over']  || RMSE:  25.234604954090546\n",
            "K= 2 : ['PctBachDeg25_Over', 'incidenceRate']  || RMSE:  22.005376080271056\n",
            "K= 3 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent']  || RMSE:  20.682886224349584\n",
            "K= 4 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace']  || RMSE:  20.29506359141554\n",
            "K= 5 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace', 'PctHS18_24']  || RMSE:  20.073708116037288\n",
            "K= 6 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace', 'PctHS18_24', 'PctMarriedHouseholds']  || RMSE:  19.86510054668294\n",
            "K= 7 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace', 'PctHS18_24', 'PctMarriedHouseholds', 'PctPrivateCoverage']  || RMSE:  19.75661710994919\n",
            "K= 8 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace', 'PctHS18_24', 'PctMarriedHouseholds', 'PctPrivateCoverage', 'PctEmpPrivCoverage-5']  || RMSE:  19.591759736930037\n",
            "K= 9 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace', 'PctHS18_24', 'PctMarriedHouseholds', 'PctPrivateCoverage', 'PctEmpPrivCoverage-5', 'AvgHouseholdSize-7']  || RMSE:  19.50636741918514\n",
            "K= 10 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace', 'PctHS18_24', 'PctMarriedHouseholds', 'PctPrivateCoverage', 'PctEmpPrivCoverage-5', 'AvgHouseholdSize-7', 'PctWhite']  || RMSE:  19.463016115432016\n",
            "K= 11 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace', 'PctHS18_24', 'PctMarriedHouseholds', 'PctPrivateCoverage', 'PctEmpPrivCoverage-5', 'AvgHouseholdSize-7', 'PctWhite', 'PctBlack-2']  || RMSE:  19.368781177517523\n",
            "K= 12 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace', 'PctHS18_24', 'PctMarriedHouseholds', 'PctPrivateCoverage', 'PctEmpPrivCoverage-5', 'AvgHouseholdSize-7', 'PctWhite', 'PctBlack-2', 'BirthRate-5']  || RMSE:  19.328543372311277\n",
            "K= 13 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace', 'PctHS18_24', 'PctMarriedHouseholds', 'PctPrivateCoverage', 'PctEmpPrivCoverage-5', 'AvgHouseholdSize-7', 'PctWhite', 'PctBlack-2', 'BirthRate-5', 'PctMarriedHouseholds-2']  || RMSE:  19.289687267591553\n",
            "K= 14 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace', 'PctHS18_24', 'PctMarriedHouseholds', 'PctPrivateCoverage', 'PctEmpPrivCoverage-5', 'AvgHouseholdSize-7', 'PctWhite', 'PctBlack-2', 'BirthRate-5', 'PctMarriedHouseholds-2', 'PctBlack-5']  || RMSE:  19.262806537502616\n",
            "K= 15 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace', 'PctHS18_24', 'PctMarriedHouseholds', 'PctPrivateCoverage', 'PctEmpPrivCoverage-5', 'AvgHouseholdSize-7', 'PctWhite', 'PctBlack-2', 'BirthRate-5', 'PctMarriedHouseholds-2', 'PctBlack-5', 'PctAsian']  || RMSE:  19.238951186894493\n",
            "K= 16 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace', 'PctHS18_24', 'PctMarriedHouseholds', 'PctPrivateCoverage', 'PctEmpPrivCoverage-5', 'AvgHouseholdSize-7', 'PctWhite', 'PctBlack-2', 'BirthRate-5', 'PctMarriedHouseholds-2', 'PctBlack-5', 'PctAsian', 'studyPerCap']  || RMSE:  19.219108799001674\n",
            "K= 17 : ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace', 'PctHS18_24', 'PctMarriedHouseholds', 'PctPrivateCoverage', 'PctEmpPrivCoverage-5', 'AvgHouseholdSize-7', 'PctWhite', 'PctBlack-2', 'BirthRate-5', 'PctMarriedHouseholds-2', 'PctBlack-5', 'PctAsian', 'studyPerCap', 'PctHS18_24-2']  || RMSE:  19.198550050358655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "BApCBnrNwfco",
        "outputId": "08fdf98e-32fa-4294-ff99-600ed40f2cdf"
      },
      "source": [
        "print('Best Features from Forward Search: ',best_features)\n",
        "plt.plot(best_losses, label='Loss on Validation')\n",
        "plt.plot(scores, label='Estimation on GE')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('RMSE Loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Features from Forward Search:  ['PctBachDeg25_Over', 'incidenceRate', 'povertyPercent', 'PctOtherRace', 'PctHS18_24', 'PctMarriedHouseholds', 'PctPrivateCoverage', 'PctEmpPrivCoverage-5', 'AvgHouseholdSize-7', 'PctWhite', 'PctBlack-2', 'BirthRate-5', 'PctMarriedHouseholds-2', 'PctBlack-5', 'PctAsian', 'studyPerCap', 'PctHS18_24-2']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f777b212150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhV1bn48e97huSQEUjClACBBBkTCUQR4gheayt1utXKdaK1+mtrrXotirbX3vb3662tPmhbtX1srbbV1rai1Fp7bVUc0BYLiCKDzEMYQiAkJISM5/39sc8JSUhChnOyk5z38zz7OXuvs/c+bxJ41z5rr72WqCrGGGNih8ftAIwxxvQuS/zGGBNjLPEbY0yMscRvjDExxhK/McbEGJ/bAXRGenq6Zmdnux2GMcb0K6tXrz6kqhmty/tF4s/OzmbVqlVuh2GMMf2KiOxqq9yaeowxJsZY4jfGmBhjid8YY2JMv2jjN8Z0Xn19PcXFxdTU1LgdiuklgUCArKws/H5/p/a3xG/MAFNcXExycjLZ2dmIiNvhmChTVQ4fPkxxcTHjxo3r1DHW1GPMAFNTU0NaWpol/RghIqSlpXXpG17UEr+IjBaR5SKyQUTWi8jtofLpIvJPEVkrIqtE5MxoxWBMrLKkH1u6+veO5hV/A3CXqk4BzgJuFZEpwA+B76jqdOD+0HZ0bHkN3lkStdMbY0x/FLXEr6r7VXVNaL0S2AhkAgqkhHZLBfZFKwZ2vAnL/wdqq6L2EcaYkyUlJbkdQrveeustZs+e3aKsoaGB4cOHs29f2+nozTffZP78+QC89NJLPPDAA23ud6qfu7y8nMcff7xpe9++fXzuc5/rSvgR0Stt/CKSDRQAK4E7gAdFZA/wEHBvO8fcEmoKWlVaWtq9D86ZB8F62PVu9443xgw455xzDsXFxezadeKh1tdee42pU6cyatSoUx5/6aWXsnjx4m59duvEP2rUKJ5//vlunasnop74RSQJWArcoapHga8Ad6rqaOBO4Mm2jlPVJ1S1UFULMzJOGmqic8bMBt8g2Pp69443xkTM2rVrOeuss8jPz+eKK67gyJEjAPz4xz9mypQp5Ofnc8011wDOVfn06dOZPn06BQUFVFZWnnS+JUuWMG3aNKZNm8YjjzwCwM6dO5k8eTI333wzU6dO5aKLLuL48eMtjvN4PFx99dU899xzTWXPPfccCxYs4P3332f27NkUFBQwZ84cPvnkk5M+9+mnn+ZrX/saADt27GD27Nnk5eXxrW99q2mfqqoq5s2bx4wZM8jLy+NPf/oTAIsXL2bbtm1Mnz6dRYsWsXPnTqZNmwY4N+W/8IUvkJeXR0FBAcuXL2/6vCuvvJKLL76YCRMmcPfdd3fvD9BMVLtziogfJ+k/q6ovhIpvBG4Prf8R+EXUAvAHIPts2GaJ38Sm7/x5PRv2HY3oOaeMSuHbn53a5eNuuOEGfvKTn3Deeedx//33853vfIdHHnmEBx54gB07dhAfH095eTkADz30EI899hhFRUVUVVURCARanGv16tU89dRTrFy5ElVl1qxZnHfeeQwZMoQtW7bwu9/9jp///OdcffXVLF26lOuuu67F8QsWLODmm2/mnnvuoba2lldeeYUlS5bg8/l455138Pl8vPbaa9x3330sXbq03Z/p9ttv5ytf+Qo33HADjz32WFN5IBDgxRdfJCUlhUOHDnHWWWdx6aWX8sADD/Dxxx+zdu1awKmowh577DFEhHXr1rFp0yYuuugiNm/eDDiV5gcffEB8fDwTJ07ktttuY/To0V3+G4RFs1eP4FzNb1TV5ndY9wHnhdbnAluiFQMAufPg8FY40uZYRcaYXlBRUUF5eTnnnef817/xxht5++23AcjPz+faa6/lmWeewedzrkWLior4z//8T3784x9TXl7eVB62YsUKrrjiChITE0lKSuLKK6/knXfeAWDcuHFMnz4dgJkzZ7ZIrmGFhYVUVVXxySef8Ne//pVZs2YxdOhQKioquOqqq5g2bRp33nkn69ev7/Dnevfdd1mwYAEA119/fVO5qnLfffeRn5/PhRdeyN69eykpKenwXCtWrGiqoCZNmsTYsWObEv+8efNITU0lEAgwZcqUFs1U3RHNK/4i4HpgnYisDZXdB9wM/EhEfEANcEsUY3Da+cG56i/8YlQ/ypi+pjtX5r3tL3/5C2+//TZ//vOf+d73vse6detYvHgxl1xyCa+88gpFRUW8+uqrTJo0qVPni4+Pb1r3er0nNfWELViwgOeee46NGzc2Je//+q//4oILLuDFF19k586dnH/++af8vLa6Uj777LOUlpayevVq/H4/2dnZPXqSuvXP1NDQ0O1zQXR79axQVVHVfFWdHlpeCZXPVNXTVXWWqq6OVgwApE+AlCxr5zfGRampqQwZMqTpqvw3v/kN5513HsFgkD179nDBBRfwgx/8gIqKCqqqqti2bRt5eXncc889nHHGGWzatKnF+c455xyWLVtGdXU1x44d48UXX+Scc87pUkwLFizgmWee4Y033uCyyy4DnG8mmZmZgNO2fipFRUVN9wqeffbZpvKKigqGDRuG3+9n+fLlTVfoycnJbd6vCP9M4XNs3ryZ3bt3M3HixC79TJ018J/cFYHcubDjbWisdzsaY2JCdXU1WVlZTcuSJUv41a9+xaJFi8jPz2ft2rXcf//9NDY2ct111zXd0Pz617/O4MGDeeSRR5g2bRr5+fn4/X4+/elPtzj/jBkzWLhwIWeeeSazZs3iS1/6EgUFBV2KcfLkySQmJjJ37lwSExMBuPvuu7n33nspKCjo1FX1j370Ix577DHy8vLYu3dvU/m1117LqlWryMvL49e//nXTt5W0tDSKioqYNm0aixYtanGur371qwSDQfLy8vj85z/P008/3eJKP5JEVaNy4kgqLCzUHk3Esn4Z/PFG+ML/wtjZp97fmH5s48aNTJ482e0wTC9r6+8uIqtVtbD1vgP/ih9g/HkgHtj2htuRGGOM62Ij8Q8aApmF1q3TGGOIlcQPTrfOvWugusztSIwxxlWxk/hz5gEK25e7HYkxxrgqdhJ/5gwIDIat1s5vjIltsZP4PV4Yf77Tzt8PejIZY0y0xE7iB8iZC5X74eBGtyMxZkDzer1Ng6xNnz693WGMAZYtW8aGDRuatu+//35ee+21HsfQV4ZAbsv777/P+eefz4QJE5gxYwaXXHIJ69atA+C///u/yczMbPH7C49hFCmxNedubrPhG4ZPcTcWYwawQYMGNQ1EdirLli1j/vz5TJni/J/87ne/G5EYwon/q1/9KuDeEMitlZSUcPXVV/Pb3/6WOXPmAM44PeGnlQHuvPNOvvGNb0Qthti64k/NgvSJNnyDMS5ZvHhx0xDM3/jGN3jvvfd46aWXWLRoEdOnT2fbtm0sXLiwKUFnZ2dz7733Mn36dAoLC1mzZg2f+tSnyMnJ4Wc/+xngzhDIr7/+OgUFBeTl5fHFL36R2trapni//e1vN8XSeqgJgEcffZQbb7yxKekDnH322Vx++eUR+i2fWmxd8YNz1f+vJ6H+OPgHuR2NMdH118VwYF1kzzkiDz7dftMNwPHjx5tGyAS49957ufDCC3nxxRfZtGkTIkJ5eTmDBw/m0ksvZf78+e02w4wZM4a1a9dy5513snDhQt59911qamqYNm0aX/7yl3t9COSamhoWLlzI66+/zmmnncYNN9zAT3/6U+644w4A0tPTWbNmDY8//jgPPfQQv/hFy5Hn169fz4033tjh7+/hhx/mmWeeAWDIkCFNFVOkxNYVPzjdOhtrbVYuY6Io3NQTXj7/+c83DSt800038cILL5CQkNCpc1166aUA5OXlMWvWLJKTk8nIyGgav7+3h0D+5JNPGDduHKeddhrQcohpgCuvvBJof0jo1mbNmsXkyZO5/fbbm8ruvPPOpt9dpJM+xOIV/9g54I13unXmXuh2NMZE1ymuzHuTz+fj/fff5/XXX+f555/n0Ucf5Y03Tt29OjxQmcfjaTFomcfjoaGhoc8NgRw+vr1jp06dypo1a5pGBF25ciXPP/88L7/8crdj7qrYu+KPS3CSvw3fYEyvqqqqoqKigs985jM8/PDDfPjhh0DHQxV3Rm8PgTxx4kR27tzJ1q1bgRNDTHfWrbfeytNPP817773XVFZdXd3p4yMh9hI/OO38pZugotjtSIwZkMJt/OFl8eLFVFZWMn/+fPLz8zn77LNZssSZmO+aa67hwQcfpKCggG3btnX5s3p7CORAIMBTTz3FVVddRV5eHh6Phy9/+cudjnfEiBH8/ve/59577yU3N5c5c+bw/PPPN83jC04bf/PfX2eajLoiNoZlbq1kA/x0Nlz6E5hxQ+TOa0wfYMMyxyYblvlUhk2G5JHWrdMYE5NiM/GLOE/xbn8Tgo1uR2OMMb0qNhM/OIm/phz2feB2JMZEXH9owjWR09W/d2wnfsSae8yAEwgEOHz4sCX/GKGqHD58mEAg0OljYq8ff1jCUBhV4HTrPP8et6MxJmKysrIoLi6mtLTU7VBMLwkEAmRlZXV6/9hN/OB063xnCRwvh0GD3Y7GmIjw+/2MGzfO7TBMHxa7TT3gDN+gjbDjLbcjMcaYXhPbiT+rEOJTrJ3fGBNTYjvxe/0w7lzY9obNymWMiRmxnfjB6d1TsQcObXE7EmOM6RWW+Jtm5bJJ2I0xscES/5BsGJpjo3UaY2KGJX5wrvp3roCGWrcjMcaYqLPED063zvpq2P0PtyMxxpios8QPkH02ePzWrdMYExMs8QPEJ8GYs+wGrzEmJljiD8udByUfQ+UBtyMxxpiossQfljPXebWrfmPMABe1xC8io0VkuYhsEJH1InJ7s/duE5FNofIfRiuGLhmeB4kZ1s5vjBnwojk6ZwNwl6quEZFkYLWI/B0YDlwGnK6qtSIyLIoxdJ7H41z1b30NgkFn2xhjBqCoZTdV3a+qa0LrlcBGIBP4CvCAqtaG3jsYrRi6LGceVB+GAx+6HYkxxkRNr1zWikg2UACsBE4DzhGRlSLyloic0RsxdEq4nd+ae4wxA1jUE7+IJAFLgTtU9ShO89JQ4CxgEfAHEZE2jrtFRFaJyKpem0koKQNG5NsNXmPMgBbVxC8ifpyk/6yqvhAqLgZeUMf7QBBIb32sqj6hqoWqWpiRkRHNMFvKnQd7VkLN0d77TGOM6UXR7NUjwJPARlVd0uytZcAFoX1OA+KAQ9GKo8ty5kGwAXa+43YkxhgTFdG84i8Crgfmisja0PIZ4JfAeBH5GHgOuFG1D82CMnoW+BOtnd8YM2BFrTunqq4ATmq7D7kuWp/bY744GHeODdNsjBmwrLN6W3LmwZGdULbd7UiMMSbiLPG3JTwrlzX3GGMGIEv8bRk6HgaPtW6dxpgByRJ/W0Scq/4db0NDndvRGGNMRFnib0/OPKirguL33Y7EGGMiyhJ/e8adCx6ftfMbYwYcS/ztCaRA1pnWrdMYM+BY4u9IzlzY/yFU9dJYQcYY0wss8XckNzRa5/Y3XQ3DGGMiyRJ/R0ZOh0FDrbnHGDOgDPjEX1Xb0P2DPV7IucDpz9+HhhMyxpieGNCJ/9t/+phPPfw2PRoDLmceVJVAyceRC8wYY1w0oBN/7rAk9pYfZ3dZdfdPYrNyGWMGmAGd+OfkOvO7rNjag+H+U0bCsKnWzm+MGTAGdOIfn57IyNQA72093LMT5c6F3f+EumORCcwYY1w0oBO/iDAnJ533th0iGOxhO39jHexcEbngjDHGJQM68QMU5aZxpLqeDft7MIfumNngG2Tt/MaYASEGEr/Tzv/eth608/sDkF1kwzQbYwaEAZ/4h6cEyB2WxLs9befPPgcOb4FjPTyPMca4bMAnfoCinDTe31FGXUOw+yfJKnRe966OTFDGGOOS2Ej8uekcr2/kg91Hun+SkdNBPJb4jTH9Xkwk/lnj0/AIvNuT/vzxSZAxGfauilxgxhjjgphI/KmD/ORlDebdbT1sn8+c4Vzx27g9xph+7JSJX0RyRCQ+tH6+iHxdRAZHP7TIOjs3jbV7yqmsqe/+SbIK4fgRKNseucCMMaaXdeaKfynQKCK5wBPAaOC3UY0qCopy0mkMKu/vKOv+STLtBq8xpv/rTOIPqmoDcAXwE1VdBIyMbliRN2PsEOJ9np5168yYBP4ES/zGmH6tM4m/XkQWADcCL4fK/NELKToCfi9nZA/t2YNcXh+MKoBiu8FrjOm/OpP4vwDMBr6nqjtEZBzwm+iGFR1zctPYdKCS0sra7p8kcwYc+Aga6iIXmDHG9KJTJn5V3aCqX1fV34nIECBZVX/QC7FFXFFOBIZvyCx0BmwrWRehqIwxpnd1plfPmyKSIiJDgTXAz0VkSfRDi7xpmamkBHw9G6Y5/ARvsbXzG2P6p8409aSq6lHgSuDXqjoLuDC6YUWH1yPMzkljxdZD3Z+OMSUTkobbDV5jTL/VmcTvE5GRwNWcuLnbbxXlpvdsOkYRp7nHnuA1xvRTnUn83wVeBbap6r9EZDywJbphRU94mOYedevMnAGHtzoPcxljTD/TmZu7f1TVfFX9Smh7u6r+e/RDi47x6YmMSAn0bNyeppE610QmKGOM6UWdubmbJSIvisjB0LJURLJ6I7hoEBHm5Kb1bDrGUQWAWOI3xvRLnWnqeQp4CRgVWv4cKuu3zs5N50h1PRsPdHM6xkAqpJ9m7fzGmH6pM4k/Q1WfUtWG0PI0kHGqg0RktIgsF5ENIrJeRG5v9f5dIqIikt7N2LvtRDt/D5t7ilfZSJ3GmH6nM4n/sIhcJyLe0HId0Jk7ow3AXao6BTgLuFVEpoBTKQAXAbu7G3hPDE8JkJOR2PMbvNWHoNyVH8EYY7qtM4n/izhdOQ8A+4HPAQtPdZCq7lfVNaH1SmAjkBl6+2HgbsC1y+Wzc9N7Nh1j00id1txjjOlfOtOrZ5eqXqqqGao6TFUvB24/1XHNiUg2UACsFJHLgL2q+uEpjrlFRFaJyKrS0tKufFynzOnpdIzDp4IvYDd4jTH9Tndn4Lq6szuKSBLOmP534DT/3Afcf6rjVPUJVS1U1cKMjFPeUuiys8LTMXZ3Vi6vH0aebiN1GmP6ne4mfunUTiJ+nKT/rKq+AOQA44APRWQnkAWsEZER3Yyj25qmY+zJDd7MQti/Fhp7MKuXMcb0snYTv4gMbWdJoxOJX0QEeBLYqKpLAFR1Xai5KFtVs4FiYIaqHojMj9M1RTlpfLinnKrahu6dIHMGNNTAwQ2RDcwYY6Kooyv+1cCq0GvzZRXQmcHoi4Drgbkisja0fKaH8UZUUW46DUHl/R3dbO5pGqnTmnuMMf2Hr703VHVcT06sqis4xTeD0FW/a2aGpmNcseUwcycN7/oJBo+FhDTnBu8ZN0U+QGOMiYLutvEPCAG/l8LsId2fmMVG6jTG9EMxnfjBae7p0XSMmTOh9BOo6ebwD8YY08ss8fd0OsasmYDCvg8iF5QxxkRRR7165jZbH9fqvSujGVRv6vF0jKNmOK/W3GOM6Sc6uuJ/qNn60lbvfSsKsbiix9MxJgyFoTn2BK8xpt/oKPFLO+ttbfdrPZ6O0UbqNMb0Ix0lfm1nva3tfm1OTg+nY8ycCVUH4Oi+CEZljDHR0W4/fmC8iLyEc3UfXie03aM+/n1NTkZoOsZth/iPWWO6foLmI3WmZna8rzHGuKyjxH9Zs/WHWr3XertfC0/HuHzTQYJBxePpYkvWiGngjXOae6Zcdur9jTHGRR09uftW8+3QgGvTcIZUPhjtwHpbUU46L6zZy8YDR5k6KrVrB/viYUSe3eA1xvQLHXXn/JmITA2tpwIfAr8GPhCRBb0UX68JT8fY7W6dmYVOX/5gYwSjMsaYyOvo5u45qro+tP4FYLOq5gEzcWbPGlBGpDrTMa7o7jDNmTOh/hiUbopsYMYYE2EdJf7mI3D+G7AMwK0hlHtDUU+mY7SROo0x/URHib9cROaLSAHOEMv/CyAiPmBQbwTX24pC0zGu3VPe9YOHjofAYHuC1xjT53WU+P8P8DXgKeCOZlf684C/RDswN4SnY+xWc4+I09xjN3iNMX1cu4lfVTer6sWqOl1Vn25W/qqq3tUr0fWy1EF+8jJTea+77fxZhc5sXLVVkQ3MGGMiqN3unCLy444OVNWvRz4c9xXlpvPE29upqm0gKb6jxxzakDkTNAj7P4TsougEaIwxPdRRU8+XgbOBfbQ9BeOA1KPpGDNnOq/Wzm+M6cM6uqQdCVwFfB5oAH4PPK+q3bjz2X/MHDuEOJ+Hd7d2YzrGxHRnOsa9A7ZeNMYMAB218R9W1Z+p6gU4/fgHAxtE5Ppei84FAb+XM7KH8G5P2vmLLfEbY/quU87AJSIzgNuB64C/MoCbecLm5DjTMR6q6sZ0jJmFcLQYKgfs4w7GmH6uoyEbvisiq4H/BN4CClX1JlXd0GvRuaRp+IZtPWnnH/D1ozGmn+roiv9bOM07pwPfB9aIyEcisk5EPuqV6FySl5lKcsDHu1u60dwzMh88PnuC1xjTZ3V0c3dAjbnfFV6PMHt8Gu92ZwJ2/yAYPtWu+I0xfVZHN3d3tbUAe3C6eQ5oRbnpFB85zu7D3ZiOsWmkzm6M+WOMMVHWURt/iojcKyKPishF4rgN2A5c3XshuiPczt+t4RsyZ0LtUTi0OcJRGWNMz3XUxv8bYCKwDvgSsBz4HHC5qg74aaZyMhIZnhLfveae8Eid1txjjOmDOpxzNzT+PiLyC2A/MEZVa3olMpeJCEU56by5ubTr0zGmTYD4FOcJ3oJroxekMcZ0Q0dX/PXhFVVtBIpjJemHFeWmU3asjo0HjnbtQI8HRhXYFb8xpk/qKPGfLiJHQ0slkB9eF5EuZsL+qUfTMWYVQsl6qD8e4aiMMaZnOurV41XVlNCSrKq+ZuspvRmkW0akBhifkdi9dv7MmRBsgP0D+pEHY0w/dMohG2Ld2bnprNzejekYM8M3eO1BLmNM32KJ/xTm5HRzOsbk4ZA62p7gNcb0OZb4T2F2aDrGbo3WmTnDbvAaY/ocS/ynkJrgTMfYvcRfCOW74Fg3h3g2xpgosMTfCXNy01m7p5xjtQ1dO9BG6jTG9EFRS/wiMlpElovIBhFZLyK3h8ofFJFNoZE+XxSRwdGKIVKKcsLTMZZ17cBR00E81s5vjOlTonnF3wDcpapTgLOAW0VkCvB3YJqq5gObgXujGENEFGY70zF2edyeuEQYNsV69hhj+pSoJX5V3a+qa0LrlcBGIFNV/6aq4TaTfwJZ0YohUgJ+L4Vjh/DOllJUtWsHZ850mnq6epwxxkRJr7Txi0g2UACsbPXWF3Gmc2zrmFtEZJWIrCotLY1ugJ0wP38Um0uqWLZ2b9cOzCqEmgo4vC06gRljTBdFPfGLSBKwFLhDVY82K/8mTnPQs20dp6pPqGqhqhZmZGREO8xT+vwZoykYM5jv/nkDh7syF6/d4DXG9DFRTfwi4sdJ+s+q6gvNyhcC84FrtcttJ+7weoQf/Hs+VbUNfPflLkw7nDEJ/InWzm+M6TOi2atHgCeBjaq6pFn5xcDdwKWq2o3prdxz2vBkvnp+Ln9au4/lmw527iCP1xmp03r2GGP6iGhe8RcB1wNzRWRtaPkM8CiQDPw9VPazKMYQcV+9IIcJw5L45ovrqOpsv/6smXBgHTR0oYnIGGOiJJq9elaoqqhqvqpODy2vqGquqo5uVvblaMUQDfE+Lw/8ez77j9bw0KufdO6gzEII1jvJ3xhjXGZP7nbDzLFDuOGssfzqHztZvevIqQ+wG7zGmD7EEn83Lbp4EiNTAtyz9CNqGxo73jk1E5JHWju/MaZPsMTfTUnxPr53RR5bD1bx+PJO9NEPP8hljDEus8TfAxdMGsZl00fx+Jtb2VxS2fHOmTOhbBtUd3G8H2OMiTBL/D10//wpJMX7uGfpRzQGO3gkoamdf03vBGaMMe2wxN9DaUnx3P/ZKXywu5zf/GNn+zuOKgDEmnuMMa6zxB8Bl0/P5NzTMvjhq59QfKSdZ9ICKc5TvPYErzHGZZb4I0BE+J8rpgHwrWUftz+Cp43UaYzpAyzxR0jWkAS+cdFE3vyklJc+3NfOTjOh+jAc2dmrsRljTHOW+CPoxjnZTB89mO/8eQNlx+pO3sEe5DLG9AGW+CMoPILn0eP1/N+2RvAcNhV8gyzxG2NcZYk/wiaOSOar5+fw4gd7efOTViN4en3OPLz2BK8xxkWW+KPg1rm55GQk8s0XP+ZY6xE8M2fC/g+hsd6d4IwxMc8SfxTE+7z84N/z2VdxnAdbj+CZORMaa6HkY3eCM8bEPEv8UVKYPZTrQyN4rtndbARPu8FrjHGZJf4oWvSpiYxICbB46UfUNQSdwsFjIDEDii3xG2PcYYk/ipIDfv7f5dPYXFLFT98MjeAp4lz171kJwVMM52yMMVFgiT/K5k0ezmdPH8Wjy7ewJTyC56T5zkidS78EDW309zfGmCiyxN8Lvv3ZKSTG+1j8wjqCQYUZ18O/fRfWvwDPLYC6fjXnvDGmPapOj71g0O1IOuRzO4BYkJ4Uz39dMoW7/vghz6zcxQ2zs6HodggMhpfvgN9cAf/xexg02O1QjRnYgkGnV11DLTTWtXytr4a6Y85SXw11VaHt0Hrz99vc5xjUH4NguAu3gNcPHh94/M5zPOF1j7fley22fS3fO29RaHTfyLHE30uunJHJsrV7+cFfNzFv8nAyBw+CmTdCINVp8nl6Ply3FJKHux2qMX1PXTWU7w4tu5zxro7ug4aaE8m7dSJvq0y7eV/NGwf+BIhLgrgEiEt01pNGhNZD7/kTwD/IufIP1juVQGO9cz+vabvBee1ou6H2xHYUmoOl3ZEk+5DCwkJdtar/P+26p6yaix5+m7PGD+WXC89ARJw3tr0Bz10LySPg+mUwZKy7gRrT2xrroaI4lNR3nfx6rNVT8L4ApGQ6CdcbB9548IVevX7wxTcrCy3hsqb34068euNaJnV/Yiihhxav353fSw+JyGpVLWxdblf8vWj00ATuuug0/t9fNvLSh/u4bHqm80bOXLjhT/DsVfDLTznJf2ObYSkAABJTSURBVNgkd4M1JtJUYd8HcGjLyYn96N6WV+PihdQs5yLotE85r4NDy5CxkDTc6SFnusWu+HtZY1C58vF32VxSxTVnjuams8eRNSTBebNkvdPe31gH1y51hnE2ZqB48wF48/sntpNGnEjorV9TMp02cdMj7V3xW+J3wb5yZyiHP3+4DwUuyRvJLeeOZ1pmKpTtgN9cDlWlsOC3MP58l6M1JgI2/hl+fx3kXQ3nLoLBo522cBNVlvj7oH3lx/nlih387v3dHKtrpCg3jVvOzeHcEQ3IM1fC4a3wuV/C5M+6Haox3XdwI/ziQsiYCAtfAX/A7YhihiX+PqzieD2/Xbmbp97dwcHKWiaNSObWs4Zyybo78OxbDZf+BAqucztMY7quugx+Ptfp+njLm5Ayyu2IYkp7id8e4OoDUgf5+cr5Oay4Zy4Pfi6foCq3LdvFvNI7KR58JvzpVnjvUbfDNKZrGhvg+S86vXWu/o0l/T7EEn8fEufzcFXhaF6941yeWngGw9OHMnf/V/ibngV/+yZVr3zbJmo3/cdr34bty2H+Ehgzy+1oTDN227wPEhEumDSMCyYN46Picn7+ViZlm77HNe8/wrtbdpJ+9Y+ZODLV7TCNad9Hf4B/PApn3AwzbnA7GtOKXfH3cflZg/nJtWdQdMezrBh+LUVHlrHp8Wu46Zfv8d62Q/SHezQmxuz7AF66DcaeDRd//9T7m15nN3f7meo3HiLh7f/LCpnBl47fxoTMYSyck815EzNIT4p3OzwT66oOwhPng3icm7mJ6S4HFNvsyd0BImHuNyAljaKX7+TtkT/hppq7uOuPHwIwaUQys3PSKMpJZ9b4oSQH+udj5qafaqiD31/v9OS56VVL+n2YJf7+qPALSCCVYS/cwkvDHmDDZU/x1j7hva2HQ91Cd+L1CHmZqRTlOhXBjLFDCPi9bkduBrK/3g17/uk8ezLydLejMR2wpp7+bOtrzhWWLwCZMyB9IvVpE9jUkMlbR4awfFc9a/eU0xhU4nweCscOoSg3ndk5aeRnpuLz2i0eEyGrfgkv3wlFd8C/fcftaEyIPcA1UBWvhpU/hdJP4NBmZ5jasKThNKSdxoG4MayrHcnysqG8fmgIh0khOd7PrPFDmZ2TTlFuGhOHJ58YLdSYrtj1D/jVfBh/gTOvhMe+WfYVvZ74RWQ08GtgOKDAE6r6IxEZCvweyAZ2Aler6pGOzmWJv5OCjc545Yc2Q+kmpzIIL3WVTbvVxaWyzz+Wj2pHsPb4cLZoJocHjWP8+AkUTchg1rihjEtPtIrAnFpFsXMzNz4Fbn7DJhPqY9xI/COBkaq6RkSSgdXA5cBCoExVHxCRxcAQVb2no3NZ4u8hVWfSikPhimATlG6G0o1w/ESdW8UgtgVHUqzpHPGPIC4tm4ysHMZNmMLY8RORgD07YJqpP+4MI354O9z8ujMWj+lTer1Xj6ruB/aH1itFZCOQCVwGnB/a7VfAm0CHid/0kAikZjpLztwT5apw7FCoQthE4sFNTDiwmeyyXQyqXktcaR2UAh84ux/zJFGTmIl/6FiSR4xHBo+B1NEweIyzDBpiY6THClV46euw/yNY8DtL+v1Mr/TqEZFsoABYCQwPVQoAB3Cagto65hbgFoAxY8ZEP8hYJAJJGc6SfTYCJITfU0WrDlKyeyvbt23gcPE26g/vILW8hMyKjYze9TaJ1LQ8X1xSqCIIVQZNlcJY5zUx3SqGgeIfj8K6P8AF34KJn3Y7GtNFUb+5KyJJwFvA91T1BREpV9XBzd4/oqpDOjqHNfX0HXvLj7Ny+2He336YDTt201i2iyw5xHj/YQpSKsmNL2dEsIRB1fuQmvKWB/sGnagUmi+podekYVYx9AdbX4dnP+cMF37Vr+xv1oe58gCXiPiBpcCzqvpCqLhEREaq6v7QfYCD7Z/B9DWZgwdx5YwsrpyRBZxOydEaVu4oY+X2w/xwRxlbi6sAGOT3MmuUj9OTK8mNK2Os9xDDgyWk1pUQX1WM7F0Dx8tantwXaPmNofm3hZRREBjszH9qicY9h7fB81+AjMlw2eP2t+inonlzV3Da8MtU9Y5m5Q8Ch5vd3B2qqnd3dC674u8/DlXV8q8dZazcUcZHxeXsOXKc0sraFvvE+TxkDRlETqowLbGcCXFHGO0pZXjwIKl1B4irLEbKd0P1oZM/QLwQSO3+Epdkyaq7aiudCVWqSpzhGIZkuxyQORU3evWcDbwDrAOCoeL7cNr5/wCMAXbhdOcsa/MkIZb4+7ea+kaKjxxnz5Fqisuqm9b3lB2n+Eg1R6rrW+yfGOcla0gCOYNhasJRcuLKGOMrJ3NQHSlUI7UVUFMBNUdDr82W+mMdByMeSEiDtFxInwDpp0H6RGd98Bjrg96eYBD+cD188le4/gWbErSfcKNXzwqgvUuredH6XNP3BPxecoclkTssqc33K2vqKT5y3KkQyqqbKoXtR6p5e6efqto0IA2AoYlxTB2VwtRRqUyb4LyOHZqAxxP6p9ZYH6oQyk+uFMLLsYNwaCtsegWqf30iEF+gVYUQWtJyIS7h5MBjyds/hE0vw8UPWNIfAGysHuO65ICfySP9TB6ZctJ7qkp5dT3bDx1jw74KPt57lI/3VfDkiu3UNzrfVpPifUwZmcLUzBSmjUplamYKuRnjOjckRXWZ88BbeCndDPvWwoY/gQZP7Jc6xqkQMia2/KYQHoisvhrqqp1vHHXVoe1jzhJeb/3atB46rj7US0o8oeYocV4l9HOcVCbtl4nHadaKTzrxGp/Sqiz55G1vGwP7bXwZ3vw+nP4fMOvLnfyrmr7Mhmww/VJdQ5DNJZWs31fB+n1H+XhvBRv2H6Wm3knW8T4Pk0amMHWUUxlMy0zhtOHJnR+orr4Gyra3rBQObYZDW5xkHeaNh8Y6nIfTO0m8TqKNSwB/Qug18cQk5KonKh1V59wabLZ+qjIg2BCqXCqhtgqC9W1FcjJvfMuKID7Z6as/bJJNlN4P2Vg9ZsBrDCo7DlU53wr2hiqEfRVU1jQA4PMIucOSmDwyhRGpAYYnxzMsJcDwlHiGJQcYlhJPvO8UFUMwCEf3nqgEjhY7yduf4PQ4ap7I48Jlic2SfCJ443r/BnNDrVMBhCuCuqrObdcedWKe/7DNmdsPWeI3MUlV2VN2nPX7Kvg41FS0paSSg5W1NARP/rc/JMHfVAkMb1YpDE8JVxIBMpLiifPZyKam77OJWExMEhHGpCUwJi2BT+eNbCoPBpUj1XWUHK2lpLKGg0drOBhaLzlay8GjNWw9WMXByloa26gghibGMSw5nozkeJLifSTG+0Kv3hPrcT6SAr5m7zvvJYbe83qsW6lxhyV+E5M8HiEtKZ60pHimcPJN5bDGoFJ2rI6SozWUVtZScrSmWWVRS2lVLfsrajhW20BVbQPHahtoo55o0yC/t0WFkBLwMyI1wIjUACNTA4xMHRR6DTA0Mc5GSzURY4nfmA54PUJG6Mq+M1SVmvpgUyUQfj1W10BVbaOz3qy8eVnF8Xr+tbOMkqM1TT2WwuJ8HkakBJoqgpGDnUphREqAUYMHMSI1QJpVDqaTLPEbE0EiwqA4L4PivJ2uLFoLBpVDx2rZX17D/ooaDlQcZ39FTWg5zqpdRyhZt//kysHrafrGMDwlEGpu8pIQ+laREOc0RSXEOU1NCfFekuJ9JMR5m7bjvB6rPGKAJX5j+hiPR5wbzMkBTh/d9j7hyuFAuEIoP87+ozXsL6/hQEUN64rLqaptpLqugeq6xk5/ts8jJMSFKoRwxRGqMAbFndhOiPOSEO9UGIOaVRwJoearhPB+oTKb5rNvscRvTD/UvHLIz+p432BQOV7fyLG6BqprnddjtS23q2sbOFbnVBTHQs1P1XXhfRvYX1HP8bqWx3T2XgY4TVXhSiPe7yHe5yXe53EWf7N1n5dA+H3/iTJnv5OPi/M65XFepywutMT7vM6614PfK/YtphVL/MYMcB6PNPUmIjky51RVahuCTuVQ2+BULOHKomn7REVSXX+iwqhrCFLbEKSmvpHahiAVx+uprW9sKq9taKS23lmvawyeOphTEAG/19NUYcR526kgfB7ivILf62la4nyttsPv+1ptt3F8XNM+Hvyh88Q1K/d7T5R5ermHlyV+Y0yXiQgBv5eA38vQxLiofU4wqNQ1nqgknAqhsaniqGsIUtsYpK7hxFLbEKSuoZG6xpZl4YrkRJlzfLis+ngj9Q1B6hvDi/PZ9Y3BULlGpCJqi9cjTd9OTlQMzvb3r8znzHFDI/p5lviNMX2WxyMEPN7OD7URZapKQ1BDlUGziiG01IXKGhqDofe0qTIJVzD1jdq0f23riia8b8OJsqT4yKdpS/zGGNNJItLUREP0vuhEnd1qN8aYGGOJ3xhjYowlfmOMiTGW+I0xJsZY4jfGmBhjid8YY2KMJX5jjIkxlviNMSbG9IupF0WkFNjVzcPTgUMRDCdSLK6usbi6xuLqmr4aF/QstrGqmtG6sF8k/p4QkVVtzTnpNourayyurrG4uqavxgXRic2aeowxJsZY4jfGmBgTC4n/CbcDaIfF1TUWV9dYXF3TV+OCKMQ24Nv4jTHGtBQLV/zGGGOascRvjDExZkAnfhG5WEQ+EZGtIrLY7XgARGS0iCwXkQ0isl5Ebnc7puZExCsiH4jIy27HEiYig0XkeRHZJCIbRWS22zEBiMidob/hxyLyOxEJuBTHL0XkoIh83KxsqIj8XUS2hF6H9JG4Hgz9HT8SkRdFZHBfiKvZe3eJiIpIel+JS0RuC/3O1ovIDyPxWQM28YuIF3gM+DQwBVggIlPcjQqABuAuVZ0CnAXc2kfiCrsd2Oh2EK38CPhfVZ0EnE4fiE9EMoGvA4WqOg3wAte4FM7TwMWtyhYDr6vqBOD10HZve5qT4/o7ME1V84HNwL29HRRtx4WIjAYuAnb3dkAhT9MqLhG5ALgMOF1VpwIPReKDBmziB84EtqrqdlWtA57D+QW6SlX3q+qa0HolThLLdDcqh4hkAZcAv3A7ljARSQXOBZ4EUNU6VS13N6omPmCQiPiABGCfG0Go6ttAWaviy4BfhdZ/BVzeq0HRdlyq+jdVbQht/hPI6gtxhTwM3A240uOlnbi+AjygqrWhfQ5G4rMGcuLPBPY02y6mjyTYMBHJBgqAle5G0uQRnH/4QbcDaWYcUAo8FWqC+oWIJLodlKruxbn62g3sBypU9W/uRtXCcFXdH1o/AAx3M5h2fBH4q9tBAIjIZcBeVf3Q7VhaOQ04R0RWishbInJGJE46kBN/nyYiScBS4A5VPdoH4pkPHFTV1W7H0ooPmAH8VFULgGO402zRQqjN/DKcimkUkCgi17kbVdvU6bPdp/pti8g3cZo9n+0DsSQA9wH3ux1LG3zAUJxm4UXAH0REenrSgZz49wKjm21nhcpcJyJ+nKT/rKq+4HY8IUXApSKyE6dZbK6IPONuSIDzTa1YVcPfip7HqQjcdiGwQ1VLVbUeeAGY43JMzZWIyEiA0GtEmggiQUQWAvOBa7VvPEiUg1OBfxj6958FrBGREa5G5SgGXlDH+zjfxnt843kgJ/5/ARNEZJyIxOHceHvJ5ZgI1dZPAhtVdYnb8YSp6r2qmqWq2Ti/qzdU1fUrWFU9AOwRkYmhonnABhdDCtsNnCUiCaG/6Tz6wE3nZl4Cbgyt3wj8ycVYmojIxTjNiZeqarXb8QCo6jpVHaaq2aF//8XAjNC/PbctAy4AEJHTgDgiMIrogE38oRtIXwNexfkP+QdVXe9uVIBzZX09zhX12tDyGbeD6uNuA54VkY+A6cD/uBwPoW8gzwNrgHU4/5dceexfRH4H/AOYKCLFInIT8ADwbyKyBefbyQN9JK5HgWTg76F/+z/rI3G5rp24fgmMD3XxfA64MRLfkmzIBmOMiTED9orfGGNM2yzxG2NMjLHEb4wxMcYSvzHGxBhL/MYYE2Ms8RvTDSJS1Wz9MyKyWUTGuhmTMZ3lczsAY/ozEZkH/Bj4lKrucjseYzrDEr8x3SQi5wI/Bz6jqtvcjseYzrIHuIzpBhGpByqB81X1I7fjMaYrrI3fmO6pB94D+sTj/sZ0hSV+Y7onCFwNnCki97kdjDFdYW38xnSTqlaLyCXAOyJSoqpPuh2TMZ1hid+YHlDVstBQw2+LSKmquj70tzGnYjd3jTEmxlgbvzHGxBhL/MYYE2Ms8RtjTIyxxG+MMTHGEr8xxsQYS/zGGBNjLPEbY0yM+f+aiLEPsHDnggAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lGjXtuCwfco",
        "outputId": "fafc1d49-740d-4ffa-a4cb-a0492eaebead"
      },
      "source": [
        "### Make prediction on our Test Set with the new set of features\n",
        "test_pred = lr.predict(X_test[best_features])\n",
        "\n",
        "### Compute the RMSE\n",
        "rmse = np.sqrt(mean_squared_error(test_pred, Y_test))\n",
        "print('RMSE Test Loss: ', rmse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE Test Loss:  36.396990333003274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xoxl2R6mwfcp"
      },
      "source": [
        "## 5-Backward Search\n",
        "\n",
        "Backward search\n",
        "starts off with F = {1, . . . , d} as the set of all features\n",
        "1. Train the model with all the features\n",
        "\n",
        "2. repeatedly delete features one at a time (evaluating single-feature deletions in a similar manner to how forward search evaluates single-feature additions) until F = âˆ…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crBiCb04wfcp",
        "scrolled": true
      },
      "source": [
        "def backward_search(model, X, Y, K):\n",
        "    \n",
        "    ### get the features of the data\n",
        "    features = X.columns.tolist()\n",
        "    \n",
        "    rmse_losses = []\n",
        "    best_losses = []\n",
        "    best_features = []\n",
        "    subset_feature = features\n",
        "    all_scores = []\n",
        "    \n",
        "    while len(subset_feature) > K:\n",
        "        all_poss_features = []\n",
        "        rmse_losses = []\n",
        "        for feature in features:\n",
        "            subset_feature = features.copy()\n",
        "            subset_feature.remove(feature)\n",
        "\n",
        "            # Extract X with the subset of new features\n",
        "            new_data = X[subset_feature]\n",
        "            \n",
        "            #split the new_data in train and validation(test_size = 20%)\n",
        "            x_train, x_val, y_train, y_val = train_test_split(new_data.values,Y.values,test_size=0.2,random_state=120)\n",
        "\n",
        "            y_train = y_train.reshape(-1, 1)\n",
        "            y_val = y_val.reshape(-1, 1)\n",
        "\n",
        "            #Cross validation using the KFold_train function\n",
        "            scores= KFold_train(model, x_train, y_train, kf)\n",
        "            \n",
        "             # Fit the model on the training set\n",
        "            #None ## Replace with your code\n",
        "            model.fit(x_train, y_train)\n",
        "            #Make Prediction on the validation set\n",
        "            ypred = model.predict(x_val)\n",
        "\n",
        "            #Calculate the RMSE on the validation set\n",
        "            rmse_loss = np.sqrt(mean_squared_error(ypred, y_val))\n",
        "\n",
        "            rmse_losses.append(rmse_loss)\n",
        "            all_poss_features.append(subset_feature)\n",
        "    \n",
        "        #Remove the feature f_i that, when eliminated, gives the best RMSE so far.\n",
        "        #ie if F-{f_i} gives the best RMSE so far, f_i will not be considered again on next iterations\n",
        "        feature_to_eliminate = features[np.argmin(rmse_losses)]\n",
        "        \n",
        "        #remove that feature from the initial set of input features\n",
        "         ### replace with your code###\n",
        "        features.remove(feature_to_eliminate)\n",
        "          \n",
        "        best_features.append(all_poss_features[np.argmin(rmse_losses)])\n",
        "        all_scores.append(np.mean(scores))\n",
        "        best_losses.append(np.min(rmse_losses))\n",
        "    return best_features,all_scores, best_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "EJ41P3Tswfcp",
        "outputId": "529de275-8ffc-46b5-dd34-df73b5a58ced"
      },
      "source": [
        "K = 250\n",
        "lr = LinearRegression()\n",
        "best_features, best_losses = backward_search(lr, X_train, Y_train, K)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b4de019c2842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL_MXmO8wfcq"
      },
      "source": [
        "print('Best Features from Backward Search: ', best_features)\n",
        "plt.plot(best_losses, label='Loss on Validation')\n",
        "plt.plot(scores, label='Estimation on G.E.')\n",
        "plt.xlabel('K')\n",
        "plt.label('RMSE Loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h9Yr6wYwfcq"
      },
      "source": [
        "### Make prediction on our Test Set with the new set of features\n",
        "test_pred = None\n",
        "\n",
        "### Compute the RMSE\n",
        "rmse = None\n",
        "print('RMSE Test Loss: ', rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTnlKgQXwfcq"
      },
      "source": [
        "<b>Missing Value Ratio</b> is a filter method for Feature Selection that drops all the features that have a number of missing values greater than a certain threshold. \n",
        "Can you implement it using this previous dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZX1CyjTwfcq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}